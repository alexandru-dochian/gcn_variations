{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "datasets = [\"Cora\", \"Citeseer\", \"Pubmed\"]\n",
    "datasets_root = \"./data\"\n",
    "default_tensor_data_type = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.set_default_dtype(default_tensor_data_type)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataset_name = 'Cora'\n",
    "\n",
    "# Download the dataset\n",
    "dataset = Planetoid(root='./data', name=dataset_name)\n",
    "\n",
    "# Access the data\n",
    "data = dataset[0]\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(f'Dataset: {dataset_name}')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of features: {data.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "# Save the dataset\n",
    "torch.save(data, f'./data/{dataset_name.lower()}_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_A(edge_index: torch.Tensor):\n",
    "    # assuming edge_index has shape (2, num_edges)\n",
    "    assert edge_index.shape[0] == 2\n",
    "\n",
    "    num_nodes = torch.max(edge_index).item() + 1\n",
    "    \n",
    "    A = torch.zeros((num_nodes, num_nodes))\n",
    "    A[edge_index[0], edge_index[1]] = 1.0\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = data.train_mask\n",
    "val_mask = data.test_mask\n",
    "num_features = data.x.shape[1]\n",
    "num_labels = len(torch.unique(data.y))\n",
    "A = create_A(data.edge_index).to(device=device, dtype= default_tensor_data_type)\n",
    "X = data.x.to(device=device, dtype= default_tensor_data_type)\n",
    "y = data.y.to(device=device, dtype= default_tensor_data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def normalize_A(A: torch.Tensor):\n",
    "    degree = torch.diag(torch.sum(A, dim=1))\n",
    "\n",
    "    # convert degree to torch.float32 for inverse to work\n",
    "    degree_hat = torch.sqrt(\n",
    "        # degree\n",
    "        torch.inverse(\n",
    "            degree.to(device=device, dtype=torch.float32)\n",
    "        )\n",
    "    ).to(device=device, dtype=default_tensor_data_type)\n",
    "    \n",
    "    return degree_hat @ A @ degree_hat\n",
    "\n",
    "def add_self_loops(A:torch.Tensor, factor: int):\n",
    "    num_nodes = A.shape[0]\n",
    "    self_loops_mask: torch.Tensor = torch.diag(torch.ones(num_nodes)) * factor\n",
    "    return A + self_loops_mask.to(device=device, dtype=default_tensor_data_type)\n",
    "\n",
    "class GraphConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super(GraphConvolutionLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(in_features, out_features).to(\n",
    "                device=device, dtype=default_tensor_data_type\n",
    "            )\n",
    "        )\n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor):\n",
    "        A = A.to(device=device, dtype= default_tensor_data_type)\n",
    "        X = X.to(device=device, dtype= default_tensor_data_type)\n",
    "        self.weight = self.weight.to(device=device, dtype= default_tensor_data_type)\n",
    "        return A @ X @ self.weight\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, output_classes, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.layer_1 = GraphConvolutionLayer(in_features, hidden_features)\n",
    "        self.layer_2 = GraphConvolutionLayer(hidden_features, output_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        A = normalize_A(A)\n",
    "        A = add_self_loops(A, 2)\n",
    "        H = F.relu(\n",
    "            self.dropout(\n",
    "                self.layer_1(X, A)\n",
    "            )\n",
    "        )\n",
    "        Y = self.layer_2(H, A)\n",
    "        Y = F.log_softmax(Y, dim=1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    predicted_labels = torch.argmax(y_pred, dim=1)\n",
    "    correct_predictions = (predicted_labels == y_true).sum().item()\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "def compute_loss(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    \n",
    "    # target to torch.long\n",
    "    return loss_function(y_pred, y_true.to(dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_A(A):\n",
    "    degrees = torch.sum(A, 1)\n",
    "    A = A / degrees\n",
    "    return A\n",
    "\n",
    "def top_k_graph(scores, A, H, k):\n",
    "    num_nodes = A.shape[0]\n",
    "    values, idx = torch.topk(scores, max(2, int(k*num_nodes)))\n",
    "    new_H = H[idx, :]\n",
    "    values = torch.unsqueeze(values, -1)\n",
    "    new_H = torch.mul(new_H, values)\n",
    "    \n",
    "    un_A = A.bool().float().to(default_tensor_data_type)\n",
    "    un_A = torch.matmul(un_A, un_A).bool().float().to(default_tensor_data_type)\n",
    "    un_A = un_A[idx, :]\n",
    "    un_A = un_A[:, idx]\n",
    "    \n",
    "    A = norm_A(un_A)\n",
    "    \n",
    "    return A, new_H, idx\n",
    "\n",
    "class Pool(nn.Module):\n",
    "    def __init__(self, k, in_dim, p):\n",
    "        super(Pool, self).__init__()\n",
    "        self.k = k\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, A, H):\n",
    "        Z = self.drop(H)\n",
    "        weights = self.proj(Z).squeeze()\n",
    "        scores = self.sigmoid(weights)\n",
    "        return top_k_graph(scores, A, H, self.k)\n",
    "\n",
    "\n",
    "class Unpool(nn.Module):\n",
    "    def forward(self, A, H, idx):\n",
    "        num_nodes = A.shape[0]\n",
    "        num_features = H.shape[1]\n",
    "        new_H = torch.zeros(\n",
    "            [num_nodes, num_features],\n",
    "            device=H.device\n",
    "        )\n",
    "        new_H[idx] = H\n",
    "        return A, new_H\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, output_classes, dropout_prob=0.8):\n",
    "        super().__init__()\n",
    "        k = 0.5\n",
    "        self.layer_1 = GraphConvolutionLayer(in_features, hidden_features)\n",
    "        self.pool = Pool(k=k, in_dim=hidden_features, p =dropout_prob)\n",
    "        self.middle_gcn = GraphConvolutionLayer(hidden_features, hidden_features)\n",
    "        self.unpool = Unpool()\n",
    "        self.layer_2 = GraphConvolutionLayer(hidden_features, output_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        A = normalize_A(A)\n",
    "        A = add_self_loops(A, 2)\n",
    "        H = F.relu(self.dropout(self.layer_1(X, A)))\n",
    "        \n",
    "        small_A, small_H, idx = self.pool(A, H)\n",
    "        \n",
    "        small_H = F.relu(self.dropout(self.middle_gcn(small_H, small_A)))\n",
    "\n",
    "        A, H_FINAL = self.unpool(A, small_H, idx)\n",
    "        \n",
    "        Y = F.log_softmax(self.layer_2(H + H_FINAL, A), dim=1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 - \n",
      "Train loss: 0.0822\tTrain acc: 0.9786\n",
      "Val loss: 2.7001\tTest acc: 0.5330\n",
      "\n",
      "Epoch  20 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.2176\tTest acc: 0.6770\n",
      "\n",
      "Epoch  30 - \n",
      "Train loss: 0.0022\tTrain acc: 1.0000\n",
      "Val loss: 2.1165\tTest acc: 0.7180\n",
      "\n",
      "Epoch  40 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.3103\tTest acc: 0.7160\n",
      "\n",
      "Epoch  50 - \n",
      "Train loss: 0.0003\tTrain acc: 1.0000\n",
      "Val loss: 2.3399\tTest acc: 0.7150\n",
      "\n",
      "Epoch  60 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.1251\tTest acc: 0.7200\n",
      "\n",
      "Epoch  70 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.3146\tTest acc: 0.7220\n",
      "\n",
      "Epoch  80 - \n",
      "Train loss: 0.0001\tTrain acc: 1.0000\n",
      "Val loss: 2.1181\tTest acc: 0.7240\n",
      "\n",
      "Epoch  90 - \n",
      "Train loss: 0.0121\tTrain acc: 0.9929\n",
      "Val loss: 2.3027\tTest acc: 0.7120\n",
      "\n",
      "Epoch 100 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.2652\tTest acc: 0.7230\n",
      "\n",
      "Epoch 110 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.3172\tTest acc: 0.7240\n",
      "\n",
      "Epoch 120 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.4346\tTest acc: 0.7040\n",
      "\n",
      "Epoch 130 - \n",
      "Train loss: 0.0009\tTrain acc: 1.0000\n",
      "Val loss: 2.4747\tTest acc: 0.7140\n",
      "\n",
      "Epoch 140 - \n",
      "Train loss: 0.0182\tTrain acc: 0.9929\n",
      "Val loss: 2.8447\tTest acc: 0.6890\n",
      "\n",
      "Epoch 150 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 2.6179\tTest acc: 0.7130\n",
      "\n",
      "Epoch 160 - \n",
      "Train loss: 0.0105\tTrain acc: 0.9929\n",
      "Val loss: 2.9156\tTest acc: 0.7140\n",
      "\n",
      "Epoch 170 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 3.3622\tTest acc: 0.6880\n",
      "\n",
      "Epoch 180 - \n",
      "Train loss: 0.2544\tTrain acc: 0.9929\n",
      "Val loss: 2.9605\tTest acc: 0.6990\n",
      "\n",
      "Epoch 190 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 3.5372\tTest acc: 0.6910\n",
      "\n",
      "Epoch 200 - \n",
      "Train loss: 0.0000\tTrain acc: 1.0000\n",
      "Val loss: 3.6799\tTest acc: 0.6730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epoch = 200\n",
    "hidden_features = 1000\n",
    "\n",
    "model = GraphUnet(num_features, hidden_features, num_labels).to(device)\n",
    "# set optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate\n",
    ")\n",
    "loss_train_history = []\n",
    "loss_val_history = []\n",
    "acc_train_history = []\n",
    "acc_val_history = []\n",
    "\n",
    "for epoch in range(1, num_epoch+1):\n",
    "    # allow model parameters to be learned   \n",
    "    model.train()         \n",
    "\n",
    "    y_pred = model(X, A)\n",
    "\n",
    "    # we will compute the loss only with respect to train data\n",
    "    y_true_train: torch.Tensor = y[train_mask] \n",
    "    y_pred_train: torch.Tensor = y_pred[train_mask]\n",
    "    loss_train = compute_loss(y_true_train, y_pred_train)\n",
    "\n",
    "    acc_train = compute_accuracy(y_true_train, y_pred_train)\n",
    "    \n",
    "    loss_train_history.append(loss_train.item())\n",
    "    acc_train_history.append(acc_train)\n",
    "\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_train.detach()\n",
    "    \n",
    "    ## model performance on validation data\n",
    "    with torch.no_grad():\n",
    "        y_true_val = y[val_mask]\n",
    "        y_pred_val = y_pred[val_mask]\n",
    "\n",
    "        loss_val = compute_loss(y_true_val, y_pred_val)\n",
    "        acc_val = compute_accuracy(y_true_val, y_pred_val)\n",
    "        loss_val_history.append(loss_val.item())\n",
    "        acc_val_history.append(acc_val)\n",
    "\n",
    "        # just making sure\n",
    "        loss_val.detach()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch:3d} - ', end='')\n",
    "        print()\n",
    "        print(f'Train loss: {loss_train:0.4f}\\tTrain acc: {acc_train:0.4f}')\n",
    "        print(f'Val loss: {loss_val:0.4f}\\tTest acc: {acc_val:0.4f}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
